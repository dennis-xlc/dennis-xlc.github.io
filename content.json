{"pages":[{"title":"","text":"个人简介 分享很喜欢的老罗的一段话： “每一个生命来到世间都注定改变世界，别无选择。要么变得好一点，要么变得坏一点。你如果走进社会为了生存为了什么不要脸的理由，变成了一个恶心的成年人社会中的一员，那你就把这个世界变得恶心了一点点。如果你一生刚正不阿，如果你一生耿直，没有做任何恶心的事情，没做对别人有害的事情，一辈子拼了老命勉强把自己身边的几个人照顾好了，没有成名没有发财，没有成就伟大的事业，然后耿着脖子一生正直，到了七八十岁耿着脖子去世了。你这一生是不是没有改变世界？你还是改变世界了，你把这个世界变得美好了一点点。因为世界上又多了一个好人。“ 善恶终有报,天道好轮回。不信抬头看,苍天饶过谁。无论何时何地，我们都要保持一颗积极乐观、善良感恩的心。但行好事莫问前程，永远年轻，永远热内盈眶，永远保持正能量。💪💪💪💪💪💪冲鸭！！！！ -&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;个人信息：计算机科学与技术专业从事JAVA后端开发码畜一枚坚信代码改变世界 博客信息 网站采用的Icarus主题 追求尽可能的简洁，清晰，易用。 在Icarus主题之上进行了部分修改。 更新日志：–2020.09.20：icarus4.0适配–2020.01.18：icarus3.0适配–2019.11.17：增加深色主题开关–2019.10.30：去图，精简卡片–2019.10.22：改版部分显示，优化速度–2019.10.16：文章列表加上评论数显示–2019.10.13：改版评论–2019.09.25：图片、资源接入CDN免费jsDelivr、文章加入置顶–2019.09.19：开源博客代码–2019.09.19：修改布局，拉伸布局，更宽的展示–2019.09.18：修改友链ui为一行三个，并适配移动端，暗黑模式文章增加评论链接，增加留言链接–2019.09.14：增加精简next主题–2019.09.14：利用中秋节放假，重做了首页的热门推荐、加个widget最新评论框、归档页加入文章贡献概览面板 本站推荐索引 博客主题相关 github Issue 作为博客微型数据库的应用 github page网站cdn优化加速 博客源码分享 博客换肤的一种实现方式思路 博客中gitalk最新评论的获取 博客图片上传picgo工具github图传使用 安装、部分配置icarus主题中文版 技术知识点 Java并发知识点 法律法规 法律法规数据库 中华人民共和国国旗法 中华人民共和国宪法 中华人民共和国消费者权益保护法 中华人民共和国刑事诉讼法 中华人民共和国婚姻法 中华人名共和国网络安全法 中华人民共和国劳动法 其他 网易云音乐歌单分享 计划2020计划 2019.12.31 2020-GOALS 跑两三场马拉松 2019计划 2018.12.31/21:59:00-&gt;更新于2019.12.31 2019-GOALS 购买的专业书籍至少看完一遍（并发、重构、设计模式…）-&gt; 95% 额外： 追了很多剧 总结： 有优点有缺点，没坚持下来的还是太多，追了太多剧。以后多学习，多思考！ 时间轴记录","link":"/about/index.html"},{"title":"","text":"🎈🎈微笑墙🎈🎈 彭小苒 唐艺昕 李一桐 gakki 图片搜集于互联网，侵权请留言，马上处理😊。","link":"/album/index.html"},{"title":"","text":"申请友链须知 原则上只和技术类博客交换，但不包括含有和色情、暴力、政治敏感的网站。 不和剽窃、侵权、无诚信的网站交换，优先和具有原创作品的网站交换。 申请请提供：站点名称、站点链接、站点描述、logo或头像（不要设置防盗链）。 排名不分先后，刷新后重排，更新信息后请留言告知。 会定期清理很久很久不更新的、不符合要求的友链，不再另行通知。 本站不存储友链图片，如果友链图片换了无法更新。图片裂了的会替换成默认图，需要更换的请留言告知。 本站友链信息如下，申请友链前请先添加本站信息： 网站图标：https://removeif.github.io/images/avatar.jpg 网站名称：辣椒の酱 网站地址：https://removeif.github.io 网站简介：后端开发，技术分享 加载中，稍等几秒...","link":"/friend/index.html"},{"title":"相册","text":"","link":"/galleries/index.html"},{"title":"","text":"&nbsp;&nbsp;听听音乐 音乐播放器由mePlayer提供，布局参照网友博客所作，感谢作者的辛勤付出。更多音乐分享请查看歌单。 &nbsp;&nbsp;看看视频 ->点击以下条目开始播放视频,向下滑动查看更多","link":"/media/index.html"},{"title":"","text":"来而不往非礼也畅所欲言，有留必应","link":"/message/index.html"},{"title":"音乐歌单收藏","text":"温馨提示：选择喜欢的音乐双击播放，由于版权原因部分不能播放。如果喜欢歌单收藏一下，去网易云都能播放哟！","link":"/music/index.html"},{"title":"","text":"碎碎念 碎碎念加载中，请稍等... $.getScript(\"/js/gitalk_self.min.js\", function () { var gitalk = new Gitalk({ language: 'zh-CN', clientID: 'a25421c2c50ffefdb16e', clientSecret: '55838dbb55caa4ba457e7f90d62ba0860088b8af', id: 'blog_owner_access_only', repo: 'blog_comment', owner: 'dennis-xlc', admin: \"dennis-xlc\", proxy: \"https://lvxu-cors-anywhere.herokuapp.com/https://github.com/login/oauth/access_token\", createIssueManually: true, distractionFreeMode: false }); gitalk.render('comment-container1'); });","link":"/self-talking/index.html"},{"title":"maternity-dress","text":"","link":"/galleries/maternity-dress/index.html"}],"posts":[{"title":"搭建本地模式的Spark环境","text":"作为当下最流行的开源大数据处理框架，Apache Spark（下文简称Spark）被广泛应用于分布式、大规模的数据处理（ETL）、分析、机器学习和图处理等方面。 从架构上讲，Spark采用的是主从（Master-Salve）架构。对于一个Spark应用，它会创建一个主进程（Master process）和若干个从属进程（Slave processes）。用Spark的专业术语来讲，主进程也被称为驱动器（Driver），而从属进程则被称作执行器（Executors）。驱动器负责分析、分配、调度和监控整个执行，同时负责维护应用在整个生命周期内所有的信息。而执行器则只要负责执行驱动器分配给他们的代码和向驱动器报告执行的状态。 Spark执行模式客户端模式 v.s 集群模式通常来讲，驱动器一定要运行在集群上，但是我们可以通过指定运行模式让驱动器运行在客户端机器还是集群本身。这个运行模式相应地被称为“客户端模式”（Client Mode）和“集群模式”（Cluster Mode）。客户端模式通常用在开发过程中，而集群模式则用于生产环境。关于“客户端模式”和“集群模式”的区别由读者自行探索，此处不做展开。 本地模式无论是客户端模式还是集群模式，其实都需要搭建一个多节点的集群。对于Spark的初学者而言，他们仅需要一个简单易用的环境用于学习和探索Spark的能力，这些模式都不够灵活机方便。为此Spark还提供一个用于测试或实验性质的本地模式（Local Mode）。 顾名思义，本地模式可以使得整个Spark应用（包括驱动器和执行器）都运行在同一个JVM进程中，不依赖任何资源管理进程（如YARN、Mesos），一台普通的台式机或笔记本电脑就满足需求。 安装Spark安装JDK如前面提到，本地模式是运行在一个JVM进程的，所以需要在运行的机器上装有JDK。现行的Spark版本都要求Java 8及以上版本，可参考Oracle官网下载安装。安装完毕后可在Shell执行java -version命令来验证。结果类似 java -version1234$ java -versionjava version &quot;1.8.0_231&quot;Java(TM) SE Runtime Environment (build 1.8.0_231-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode) 下载和安装Spark首先，前往Spark官网下载最新的Spark版本。截止到本文写作日期，Spark最新版本为Spark 3.0.0-preview2，如下图-1所示，但读者总是可以用相同的方法下载安装最新的3.0发布版本。 点击图-1所示的第三步的链接即可下载，或者在Shell里使用wget命令直接下载，如下所示： 下载Spark1234567891011$ wget -c https://archive.apache.org/dist/spark/spark-3.0.0-preview2/spark-3.0.0-preview2.tgz ....Length: 21811982 (21M), 21582606 (21M) remaining [application/x-gzip]Saving to: ‘spark-3.0.0-preview2.tgz’spark-3.0.0-preview2.tgz 100%[==========================================&gt;] 20.80M 119KB/s in 4m 10s2020-02-29 13:23:32 (84.4 KB/s) - ‘spark-3.0.0-preview2.tgz’ saved [21811982/21811982]...Total wall clock time: 4m 11sDownloaded: 1 files, 21M in 4m 10s (84.4 KB/s) 下载完成后，将spark-3.0.0-preview2.tgz解压放到相应目录下，如当前目录的spark目录。 解压Spark12$ mkdir spark$ tar -zxvf spark-3.0.0-preview2.tgz -C spark 查看该目录内容，是否包含下列文件： 查看Spark目录12345678910$ ls spark/spark-3.0.0-preview2CONTRIBUTING.md common graphx replLICENSE conf hadoop-cloud resource-managersNOTICE core launcher sbinR data licenses scalastyle-config.xmlREADME.md dev mllib sqlappveyor.yml docs mllib-local streamingassembly examples pom.xml toolsbin external projectbuild graph python 最后指定SPARK_HOME到环境变量中，如Linux或Mac，可以在~/.bashrc或~/.bash_profile中添加： 添加Spark环境变量12export SPARK_HOME=/Users/dennis/spark/spark-3.0.0-preview2-bin-hadoop2.7export PATH=$SPARK_HOME/bin:$PATH 至此，本地模式的Spark环境就已安装完毕！ 验证本地模式的SparkSpark提供了一个非常方便的交互式工具spark-shell，用户可以非常方便在上面执行scala代码。想要启动Spark的shell，只需在命令上上输入spark-shell： spark shell1234567891011121314151617181920$ spark-shell 127 ↵20/02/29 14:22:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).Spark context Web UI available at http://localhost:4040Spark context available as 'sc' (master = local[*], app id = local-1582957351388).Spark session available as 'spark'.Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 3.0.0-preview2 /_/Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_212)Type in expressions to have them evaluated.Type :help for more information.scala&gt; 进去后可以看到就是一个scala的cli，可以在上面运行一下scala语句和spark的API看看： play with scala12345678910111213141516171819202122232425262728scala&gt; spark.versionres1: String = 3.0.0-preview2scala&gt; val strings = spark.read.text(&quot;README.md&quot;)strings: org.apache.spark.sql.DataFrame = [value: string]scala&gt; strings.show(10, false)+--------------------------------------------------------------------------------+|value |+--------------------------------------------------------------------------------+|# Apache Spark || ||Spark is a unified analytics engine for large-scale data processing. It provides||high-level APIs in Scala, Java, Python, and R, and an optimized engine that ||supports general computation graphs for data analysis. It also supports a ||rich set of higher-level tools including Spark SQL for SQL and DataFrames, ||MLlib for machine learning, GraphX for graph processing, ||and Structured Streaming for stream processing. || ||&lt;https://spark.apache.org/&gt; |+--------------------------------------------------------------------------------+only showing top 10 rowsscala&gt; strings.count()res3: Long = 109scala&gt; 至此本地模式的Spark环境就已经搭建完毕，我们可以方便地在本地环境学习和使用Spark相关的概念和内容。 以上。","link":"/2021/02/27/setup-local-mode-spark/"}],"tags":[{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"Tutorial","slug":"Tutorial","link":"/tags/Tutorial/"}],"categories":[{"name":"Data Technology","slug":"Data-Technology","link":"/categories/Data-Technology/"},{"name":"Spark","slug":"Data-Technology/Spark","link":"/categories/Data-Technology/Spark/"}]}